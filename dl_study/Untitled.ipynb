{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "x = tf.constant([[10.]]) #Input setting (input -> matrix)\n",
    "#불변함수 지정할때는 constant, 변수 쓸 때는 variable\n",
    "print(type(x)) #텐서플로우 내 타입. 파이썬의 구조랑 별개라고 보기"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dense = Dense(units=1, activation='linear') #뉴런 몇 개 구성할 것인지 #imp. an affine function #항등함수 리니어 넣어보기로 한다.\n",
    "#dense.get_weights() 여기서 불러오면 error → 왜냐하면 텐서 1.0 첫 사상은 lazy running이기에 선언 시 아직 x와 b가 초기화가 안 된 상태.\n",
    "y_tf = dense(x) #forward propagation + params initialization z = xw+b #랜덤하게 초기화 됨.\n",
    "\n",
    "W, b = dense.get_weights() # 이 메소드로 W와 b값 가져올 수 있음.\n",
    "print(W, b, y_tf.numpy() ) # 10넣었을때 weight과 bias, 결과값 확인 #텐서타입에서 넘파이 타입으로 바꿔서 출력하는 것\n",
    "print(type(W),type(b))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1.5911273]] [0.] [[15.911273]]\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "y_man = tf.linalg.matmul(x,W) +b # 매뉴얼로 affine펑션 구현, matmul은 행렬곱인 matrix multiply\n",
    "\n",
    "print('=======Input / Weight / Bias=======')\n",
    "print('x: {}\\n{}\\n'.format(x.shape, x.numpy()))\n",
    "print('W: {}\\n{}\\n'.format(W.shape, W))\n",
    "print('b: {}\\n{}\\n'.format(b.shape, b))\n",
    "\n",
    "print('=======output=======')\n",
    "print('y(Tensorflow): {}\\n{}\\n'.format(y_tf.shape, y_tf.numpy()))\n",
    "print('y(Manual): {}\\n{}\\n'.format(y_man.shape, y_man.numpy()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=======Input / Weight / Bias=======\n",
      "x: (1, 1)\n",
      "[[10.]]\n",
      "\n",
      "W: (1, 1)\n",
      "[[1.5911273]]\n",
      "\n",
      "b: (1,)\n",
      "[0.]\n",
      "\n",
      "=======output=======\n",
      "y(Tensorflow): (1, 1)\n",
      "[[15.911273]]\n",
      "\n",
      "y(Manual): (1, 1)\n",
      "[[15.911273]]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Params Initialization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "# weight / bias setting\n",
    "W, b = (tf.constant(10.), tf.constant(20.))\n",
    "w_init, b_init = Constant(10.), Constant(20.)\n",
    "\n",
    "# 랜덤하게 초기화되는 부분을 고정시켜봄\n",
    "dense = Dense(units = 1, activation = 'linear',\n",
    "              kernel_initializer = w_init,\n",
    "              bias_initializer=b_init)\n",
    "\n",
    "y_tf = dense(x)\n",
    "W, b = dense.get_weights()\n",
    "\n",
    "print('W: {}\\n{}\\n'.format(W.shape, W))\n",
    "print('b: {}\\n{}\\n'.format(b.shape, b))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "W: (1, 1)\n",
      "[[10.]]\n",
      "\n",
      "b: (1,)\n",
      "[20.]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(y_tf.numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[120.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Affine Functions w/ n Feature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "tf.random.set_seed(10)\n",
    "#random seed 같은 기능으로 timestamp처럼 기준시 있는데 default는 now니까 안 쓰면 랜덤하게 계속 바뀜\n",
    "x = tf.random.uniform(shape=(1,10), minval=0, maxval=10)\n",
    "dense = Dense(units=1) # act_f는 현재 없으므로 linear 통과한 것과 비슷하게 나올 것임."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "x.numpy()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[6.4415097 , 8.082472  , 8.976547  , 6.3689017 , 6.270969  ,\n",
       "        9.936013  , 0.23594856, 0.36683917, 5.8605776 , 5.7403145 ]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "y_tf = dense(x)\n",
    "W, b = dense.get_weights()\n",
    "y_man = tf.linalg.matmul(x,W) + b"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print('=======Input / Weight / Bias=======')\n",
    "print('x: {}\\n{}\\n'.format(x.shape, x.numpy())) #row vector\n",
    "print('W: {}\\n{}\\n'.format(W.shape, W)) #column vector\n",
    "print('b: {}\\n{}\\n'.format(b.shape, b))\n",
    "\n",
    "print('=======output=======')\n",
    "print('y(Tensorflow): {}\\n{}\\n'.format(y_tf.shape, y_tf.numpy()))\n",
    "print('y(Manual): {}\\n{}\\n'.format(y_man.shape, y_man.numpy()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=======Input / Weight / Bias=======\n",
      "x: (1, 10)\n",
      "[[6.4415097  8.082472   8.976547   6.3689017  6.270969   9.936013\n",
      "  0.23594856 0.36683917 5.8605776  5.7403145 ]]\n",
      "\n",
      "W: (10, 1)\n",
      "[[ 0.08091772]\n",
      " [-0.10844558]\n",
      " [-0.5296768 ]\n",
      " [ 0.30087596]\n",
      " [-0.18808389]\n",
      " [ 0.29202002]\n",
      " [-0.39361012]\n",
      " [ 0.09546101]\n",
      " [-0.4453573 ]\n",
      " [ 0.6689041 ]]\n",
      "\n",
      "b: (1,)\n",
      "[0.]\n",
      "\n",
      "=======output=======\n",
      "y(Tensorflow): (1, 1)\n",
      "[[-0.29983282]]\n",
      "\n",
      "y(Manual): (1, 1)\n",
      "[[-0.29983282]]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Activation Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.math import exp, maximum\n",
    "from tensorflow.keras.layers import Dense"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "tf.random.set_seed(10)\n",
    "x = tf.random.normal(shape=(1,5)) # 중복 안되게 난수 가져오는 함수\n",
    "sigmoid = Activation('sigmoid')\n",
    "tanh = Activation('tanh')\n",
    "relu = Activation('relu')\n",
    "\n",
    "# forward propagation(tensorflow)\n",
    "# affine건너뛰고 act_f통과시키는 작업으로 진행\n",
    "y_sigmoid_tf = sigmoid(x)\n",
    "y_tanh_tf = tanh(x)\n",
    "y_relu_tf = relu(x)\n",
    "\n",
    "print('x: {}\\n{}'.format(x.shape, x.numpy()))\n",
    "print('sigmoid(Tensorflow): {}\\n{}'.format(y_sigmoid_tf.shape, y_sigmoid_tf.numpy()))\n",
    "print('tanh(Tensorflow): {}\\n{}'.format(y_tanh_tf.shape, y_tanh_tf.numpy()))\n",
    "print('relu(Tensorflow): {}\\n{}'.format(y_relu_tf.shape, y_relu.numpy()))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Activation' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d9be4eeb9781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 중복 안되게 난수 가져오는 함수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msigmoid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtanh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Activation' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Activation in Dense Layer\n",
    "from tensorflow.keras.layers import Dense\n",
    "tf.random.set_seed(10)\n",
    "x = tf.random.normal(shape=(1,5))\n",
    "\n",
    "# imp. Artificial neurons(AN)\n",
    "dense_sigmoid = Dense(units=1, activation = 'sigmoid')\n",
    "dense_tanh = Dense(units=1, activation = 'tanh')\n",
    "dense_relu = Dense(units=1, activation = 'relu')\n",
    "\n",
    "# forward propagation\n",
    "y_sigmoid = dense_sigmoid(x)\n",
    "y_tanh = dense_tanh(x)\n",
    "y_relu = dense_relu(x)\n",
    "\n",
    "print('AN with sigmoid: {}\\n{}'.format(y_sigmoid.shape, y_sigmoid.numpy()))\n",
    "print('tanh with sigmoid: {}\\n{}'.format(y_tanh.shape, y_tanh.numpy()))\n",
    "print('relu with sigmoid: {}\\n{}'.format(y_relu.shape, y_relu.numpy()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AN with sigmoid: (1, 1)\n",
      "[[0.498436]]ERROR! Session/line number was not unique in\n",
      "tanh with sigmoid: (1, 1)\n",
      "[[0.32123712]]\n",
      "relu with sigmoid: (1, 1)\n",
      "[[0.]]\n",
      " database. History logging moved to new session 396\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "39c6663c694f66742a48c32a0de310398657a941bd026b39790ca56507cab329"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}