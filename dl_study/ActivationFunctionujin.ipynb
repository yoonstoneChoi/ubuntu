{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[10.]]) #Input setting (input -> matrix)\n",
    "#불변함수 지정할때는 constant, 변수 쓸 때는 variable\n",
    "print(type(x)) #텐서플로우 내 타입. 파이썬의 구조랑 별개라고 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.6883842]] [0.] [[-6.8838415]]\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "dense = Dense(units=1, activation='linear') #뉴런 몇 개 구성할 것인지 #imp. an affine function #항등함수 리니어 넣어보기로 한다.\n",
    "#dense.get_weights() 여기서 불러오면 error → 왜냐하면 텐서 1.0 첫 사상은 lazy running이기에 선언 시 아직 x와 b가 초기화가 안 된 상태.\n",
    "y_tf = dense(x) #forward propagation + params initialization z = xw+b #랜덤하게 초기화 됨.\n",
    "\n",
    "W, b = dense.get_weights() # 이 메소드로 W와 b값 가져올 수 있음.\n",
    "print(W, b, y_tf.numpy() ) # 10넣었을때 weight과 bias, 결과값 확인 #텐서타입에서 넘파이 타입으로 바꿔서 출력하는 것\n",
    "print(type(W),type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Input / Weight / Bias=======\n",
      "x: (1, 1)\n",
      "[[10.]]\n",
      "\n",
      "W: (1, 1)\n",
      "[[-0.6883842]]\n",
      "\n",
      "b: (1,)\n",
      "[0.]\n",
      "\n",
      "=======output=======\n",
      "y(Tensorflow): (1, 1)\n",
      "[[-6.8838415]]\n",
      "\n",
      "y(Manual): (1, 1)\n",
      "[[-6.8838415]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_man = tf.linalg.matmul(x,W) +b # 매뉴얼로 affine펑션 구현, matmul은 행렬곱인 matrix multiply\n",
    "\n",
    "print('=======Input / Weight / Bias=======')\n",
    "print('x: {}\\n{}\\n'.format(x.shape, x.numpy()))\n",
    "print('W: {}\\n{}\\n'.format(W.shape, W))\n",
    "print('b: {}\\n{}\\n'.format(b.shape, b))\n",
    "\n",
    "print('=======output=======')\n",
    "print('y(Tensorflow): {}\\n{}\\n'.format(y_tf.shape, y_tf.numpy()))\n",
    "print('y(Manual): {}\\n{}\\n'.format(y_man.shape, y_man.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: (1, 1)\n",
      "[[10.]]\n",
      "\n",
      "b: (1,)\n",
      "[20.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "# weight / bias setting\n",
    "W, b = (tf.constant(10.), tf.constant(20.))\n",
    "w_init, b_init = Constant(10.), Constant(20.)\n",
    "\n",
    "# 랜덤하게 초기화되는 부분을 고정시켜봄\n",
    "dense = Dense(units = 1, activation = 'linear',\n",
    "              kernel_initializer = w_init,\n",
    "              bias_initializer=b_init)\n",
    "\n",
    "y_tf = dense(x)\n",
    "W, b = dense.get_weights()\n",
    "\n",
    "print('W: {}\\n{}\\n'.format(W.shape, W))\n",
    "print('b: {}\\n{}\\n'.format(b.shape, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[120.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_tf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine Functions w/ n Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "tf.random.set_seed(10)\n",
    "#random seed 같은 기능으로 timestamp처럼 기준시 있는데 default는 now니까 안 쓰면 랜덤하게 계속 바뀜\n",
    "x = tf.random.uniform(shape=(1,10), minval=0, maxval=10)\n",
    "dense = Dense(units=1) # act_f는 현재 없으므로 linear 통과한 것과 비슷하게 나올 것임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.4415097 , 8.082472  , 8.976547  , 6.3689017 , 6.270969  ,\n",
       "        9.936013  , 0.23594856, 0.36683917, 5.8605776 , 5.7403145 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tf = dense(x)\n",
    "W, b = dense.get_weights()\n",
    "y_man = tf.linalg.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Input / Weight / Bias=======\n",
      "x: (1, 10)\n",
      "[[6.4415097  8.082472   8.976547   6.3689017  6.270969   9.936013\n",
      "  0.23594856 0.36683917 5.8605776  5.7403145 ]]\n",
      "\n",
      "W: (10, 1)\n",
      "[[ 0.08091772]\n",
      " [-0.10844558]\n",
      " [-0.5296768 ]\n",
      " [ 0.30087596]\n",
      " [-0.18808389]\n",
      " [ 0.29202002]\n",
      " [-0.39361012]\n",
      " [ 0.09546101]\n",
      " [-0.4453573 ]\n",
      " [ 0.6689041 ]]\n",
      "\n",
      "b: (1,)\n",
      "[0.]\n",
      "\n",
      "=======output=======\n",
      "y(Tensorflow): (1, 1)\n",
      "[[-0.29983306]]\n",
      "\n",
      "y(Manual): (1, 1)\n",
      "[[-0.29983306]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('=======Input / Weight / Bias=======')\n",
    "print('x: {}\\n{}\\n'.format(x.shape, x.numpy())) #row vector\n",
    "print('W: {}\\n{}\\n'.format(W.shape, W)) #column vector\n",
    "print('b: {}\\n{}\\n'.format(b.shape, b))\n",
    "\n",
    "print('=======output=======')\n",
    "print('y(Tensorflow): {}\\n{}\\n'.format(y_tf.shape, y_tf.numpy()))\n",
    "print('y(Manual): {}\\n{}\\n'.format(y_man.shape, y_man.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.math import exp, maximum\n",
    "from tensorflow.keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (1, 5)\n",
      "[[-0.8757808   0.3356369  -0.35219625 -0.30314562 -0.03882965]]\n",
      "\n",
      "sigmoid(Tensorflow): (1, 5)\n",
      "[[0.29405287 0.5831303  0.41284996 0.4247887  0.4902938 ]]\n",
      "tanh(Tensorflow): (1, 5)\n",
      "[[-0.70429933  0.32357663 -0.33832183 -0.29418862 -0.03881015]]\n",
      "relu(Tensorflow): (1, 5)\n",
      "[[0.        0.3356369 0.        0.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(10)\n",
    "x = tf.random.normal(shape=(1,5)) # 중복 안되게 난수 가져오는 함수\n",
    "sigmoid = Activation('sigmoid')\n",
    "tanh = Activation('tanh')\n",
    "relu = Activation('relu')\n",
    "\n",
    "# forward propagation(tensorflow)\n",
    "# affine건너뛰고 act_f통과시키는 작업으로 진행\n",
    "y_sigmoid_tf = sigmoid(x)\n",
    "y_tanh_tf = tanh(x)\n",
    "y_relu_tf = relu(x)\n",
    "\n",
    "print('x: {}\\n{}\\n'.format(x.shape, x.numpy()))\n",
    "print('sigmoid(Tensorflow): {}\\n{}'.format(y_sigmoid_tf.shape, y_sigmoid_tf.numpy()))\n",
    "print('tanh(Tensorflow): {}\\n{}'.format(y_tanh_tf.shape, y_tanh_tf.numpy()))\n",
    "print('relu(Tensorflow): {}\\n{}'.format(y_relu_tf.shape, y_relu_tf.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (1, 5)\n",
      "[[-0.8757808   0.3356369  -0.35219625 -0.30314562 -0.03882965]]\n",
      "\n",
      "An with sigmoid(man): (1, 5)\n",
      "[[0.29405284 0.5831303  0.41284993 0.42478868 0.49029383]]\n",
      "An with tanh(man): (1, 5)\n",
      "[[-0.70429933  0.32357663 -0.3383218  -0.29418865 -0.03881014]]\n",
      "An with relu(man): (1, 5)\n",
      "[[0.        0.3356369 0.        0.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "# forward propagation(manual)\n",
    "y_sigmoid_man = 1 / (1+tf.math.exp(-x))\n",
    "y_tanh_man = (tf.math.exp(x) - tf.math.exp(-x)) / (tf.math.exp(x) + tf.math.exp(-x))\n",
    "y_relu_man = tf.math.maximum(x,0)\n",
    "\n",
    "print('x: {}\\n{}\\n'.format(x.shape, x.numpy()))\n",
    "print('An with sigmoid(man): {}\\n{}'.format(y_sigmoid_man.shape, y_sigmoid_man.numpy()))\n",
    "print('An with tanh(man): {}\\n{}'.format(y_tanh_man.shape, y_tanh_man.numpy()))\n",
    "print('An with relu(man): {}\\n{}'.format(y_relu_man.shape, y_relu_man.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AN with sigmoid: (1, 1)\n",
      "[[0.498436]]\n",
      "AN with tanh: (1, 1)\n",
      "[[0.32123712]]\n",
      "AN with relu: (1, 1)\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "# Activation in Dense Layer\n",
    "from tensorflow.keras.layers import Dense\n",
    "tf.random.set_seed(10)\n",
    "x = tf.random.normal(shape=(1,5))\n",
    "\n",
    "# imp. Artificial neurons(AN)\n",
    "dense_sigmoid = Dense(units=1, activation = 'sigmoid')\n",
    "dense_tanh = Dense(units=1, activation = 'tanh')\n",
    "dense_relu = Dense(units=1, activation = 'relu')\n",
    "\n",
    "# forward propagation\n",
    "y_sigmoid = dense_sigmoid(x)\n",
    "y_tanh = dense_tanh(x)\n",
    "y_relu = dense_relu(x)\n",
    "\n",
    "print('AN with sigmoid: {}\\n{}'.format(y_sigmoid.shape, y_sigmoid.numpy()))\n",
    "print('AN with tanh: {}\\n{}'.format(y_tanh.shape, y_tanh.numpy()))\n",
    "print('AN with relu: {}\\n{}'.format(y_relu.shape, y_relu.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation value(Tensorflow): (1, 1)\n",
      "[[0.498436]]\n",
      "Activation value(Manual): (1, 1)\n",
      "[[0.498436]]\n"
     ]
    }
   ],
   "source": [
    "# forward propagation(manual)\n",
    "W, b = dense_sigmoid.get_weights()\n",
    "z = tf.linalg.matmul(x,W) +b\n",
    "a = 1 / (1 + tf.exp(-z))\n",
    "\n",
    "print('Activation value(Tensorflow): {}\\n{}'.format(y_sigmoid.shape, y_sigmoid.numpy()))\n",
    "print('Activation value(Manual): {}\\n{}'.format(a.shape, a.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x:  (8, 10)\n",
      "Shape of W:  (10, 1)\n",
      "Shape of b:  (1,)\n"
     ]
    }
   ],
   "source": [
    "# Minibatches, Shapes of Dense Layers\n",
    "N, n_features = 8,10\n",
    "tf.random.set_seed(10)\n",
    "x = tf.random.normal(shape=(N, n_features))\n",
    "\n",
    "dense = Dense(units=1, activation='relu')\n",
    "y = dense(x)\n",
    "W, b = dense.get_weights()\n",
    "\n",
    "print('Shape of x: ', x.shape)\n",
    "print('Shape of W: ', W.shape)\n",
    "print('Shape of b: ', b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output(Tensorflow): (8, 1)\n",
      "[[0.46854386]\n",
      " [0.5895974 ]\n",
      " [0.10466993]\n",
      " [0.8744323 ]\n",
      " [0.7818612 ]\n",
      " [0.47610277]\n",
      " [0.33888918]\n",
      " [0.7424137 ]]\n",
      "\n",
      "Output(Manual): (8, 1)\n",
      "[[0.46854383]\n",
      " [0.5895974 ]\n",
      " [0.10466997]\n",
      " [0.8744324 ]\n",
      " [0.7818612 ]\n",
      " [0.4761027 ]\n",
      " [0.33888915]\n",
      " [0.7424137 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Output Calculations\n",
    "N, n_features = 8,10\n",
    "tf.random.set_seed(10)\n",
    "x = tf.random.normal(shape=(N, n_features))\n",
    "\n",
    "dense = Dense(units=1, activation='sigmoid')\n",
    "y_tf = dense(x)\n",
    "W, b = dense.get_weights()\n",
    "\n",
    "y_man = tf.linalg.matmul(x,W) + b\n",
    "y_man = 1 / (1+tf.math.exp(-y_man))\n",
    "\n",
    "print('Output(Tensorflow): {}\\n{}\\n'.format(y_tf.shape, y_tf.numpy()))\n",
    "print('Output(Manual): {}\\n{}\\n'.format(y_man.shape, y_man.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
